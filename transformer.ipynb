{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ed2619-0cd0-441f-8315-f87506ee3a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.1.2%2Bcpu-cp311-cp311-linux_x86_64.whl (184.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.9/184.9 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.16.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.1.2%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.0/154.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/numpy-1.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/Pillow-9.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torchvision) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, pillow, numpy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.9.0 fsspec-2023.4.0 mpmath-1.3.0 networkx-3.0 numpy-1.24.1 pillow-9.3.0 sympy-1.12 torch-2.1.2+cpu torchaudio-2.1.2+cpu torchvision-0.16.2+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576dad17-459e-4bfa-bde9-c5705cb370f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "333416ac-c11b-4ddd-a71a-a6bb372b6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Throw error if the model dimension is not divisble by the number of heads\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # Dimensions of query, key, and value to calc output\n",
    "\n",
    "        # Linear layers apply a linear transformation to incoming data\n",
    "        # nn.Linear() automatically initialize weights and biases\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query Transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key Transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value Transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Not sure what the mask does but default to zero for now\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1) # Softmax across the rows\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size() # Batch size, sequence length, and dimension of the model set to the size of the provided tensor\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # The transpose swaps the second and third dimesion... why???\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Combine the heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size() # Second value is num heads which we are concatting out\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        weighted_q = self.W_q(Q)\n",
    "        weighted_k = self.W_k(K)\n",
    "        weighted_v = self.W_v(V)\n",
    "\n",
    "        print(\"------ WEIGHTED Q SHAPE: \" + str(weighted_q.shape))\n",
    "        print(\"batch_size, seq_length, d_model; Now weighted by a linear transformation\")\n",
    "        \n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(weighted_q)\n",
    "        K = self.split_heads(weighted_k)\n",
    "        V = self.split_heads(weighted_v)\n",
    "\n",
    "        print(\"------- SPLIT HEAD Q SHAPE: \" + str(Q.shape))\n",
    "        print(\"batch_size, num_heads, seq_length, dimension_keys; Model dimiensions split across heads and matrix transposed\")\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output)) # Combine the outputs and then multiply against output weights\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e628515f-d7fe-4984-ba1c-d37c17670677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ STARTING Q SHAPE: torch.Size([2, 10, 8])\n",
      "batch_size, seq_length, d_model\n",
      "------ WEIGHTED Q SHAPE: torch.Size([2, 10, 8])\n",
      "batch_size, seq_length, d_model; Now weighted by a linear transformation\n",
      "------- SPLIT HEAD Q SHAPE: torch.Size([2, 2, 10, 4])\n",
      "batch_size, num_heads, seq_length, dimension_keys; Model dimiensions split across heads and matrix transposed\n",
      "------ FINAL OUTPUT SHAPE: torch.Size([2, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize parameters\n",
    "d_model = 8  # Model's dimension\n",
    "num_heads = 2  # Number of attention heads\n",
    "\n",
    "# Ensure d_model is divisible by num_heads\n",
    "assert d_model % num_heads == 0\n",
    "\n",
    "# Create an instance of MultiHeadAttention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Prepare dummy input data (batch_size, seq_length, d_model)\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "Q = torch.rand(batch_size, seq_length, d_model)\n",
    "K = torch.rand(batch_size, seq_length, d_model)\n",
    "V = torch.rand(batch_size, seq_length, d_model)\n",
    "\n",
    "print(\"------ STARTING Q SHAPE: \" + str(Q.shape))\n",
    "print(\"batch_size, seq_length, d_model\")\n",
    "\n",
    "# Forward pass through the MultiHeadAttention module\n",
    "# Sequence length never changes throughout the path because we are not modifying the input\n",
    "output = mha.forward(Q, K, V)\n",
    "\n",
    "# Batch Size - Number of times the operation will be repeated\n",
    "# Num Heads - Number of parallel computations\n",
    "# Sequence Length - Number of tokens in the sequence\n",
    "# Dimension Keys - Number of features representing a token; aka the size of the vector per token\n",
    "\n",
    "# Input and Output shape should always be the same size\n",
    "print(\"------ FINAL OUTPUT SHAPE: \" + str(output.shape))  # Should be [batch_size, seq_length, d_model]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
